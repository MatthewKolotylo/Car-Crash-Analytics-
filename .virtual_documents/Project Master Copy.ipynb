
# !pip install fredapi -q
# !pip install plotly -q
# !pip install opendatasets --upgrade -q
# !pip install -U scikit-learn -q
# !pip install scipy -q
# !pip install joblib --upgrade-q
# !pip install wordcloud -q


import pandas as pd
import numpy as np
import opendatasets as od
import matplotlib.pyplot as plt
from fredapi import Fred
import plotly.express as px
import joblib
from wordcloud import WordCloud




import secret





dbg = True


#ONLY NEED TO RUN THIS ONCE
data_url='https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents'
od.download(data_url, force=True)

#note use username and key in slack chat


#mac version
import os
data_dir = r'./us-accidents'
os.listdir(data_dir)



#windows version
# import os
# data_dir = r'./us-accidents'
# os.listdir(data_dir)



data_filename=data_dir+'/US_Accidents_March23.csv'


if dbg:
    car_crash_df=pd.read_csv(data_filename, nrows=200000)
else:
    car_crash_df=pd.read_csv(data_filename)






#pd.set_option('display.max_columns', None)
car_crash_df.head(5)


car_crash_df.tail(5)


car_crash_df.shape


car_crash_df.info()


car_crash_df.describe()


car_crash_df.columns


car_crash_df.dtypes





missing_percentage = car_crash_df.isna().mean() * 100
missing_percentage


#dropping columns with more than 20% missing values
car_crash_df=car_crash_df.drop(columns=['End_Lat','End_Lng','Wind_Chill(F)','Precipitation(in)'])



#Dropping columns not relevant to the analysis
car_crash_df=car_crash_df.drop(columns=['ID','Source','Airport_Code','Weather_Timestamp','Amenity','Give_Way','Junction','No_Exit','Railway','Station','Stop','Traffic_Calming','Turning_Loop','Timezone','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'])  




car_crash_df.columns


car_crash_df.drop_duplicates(inplace=True)


#adding a Year-Month Column for easier joins
car_crash_df['Year-Month'] = pd.to_datetime(car_crash_df['Start_Time'], format='%Y-%m-%d %H:%M:%S', errors='coerce').dt.to_period('M')



print(car_crash_df['Year-Month'].isna().sum())
#dropping rows with missing Year-Month, will help notebook performance
car_crash_df= car_crash_df.dropna(subset=['Year-Month'])



#fixing index due to the rows we dropped
car_crash_df.reset_index(drop=True, inplace=True)


car_crash_df.shape





#using our super secret key to acess fred api
fred = Fred(api_key=secret.fred_key)





# pop_search= fred.search('Resident Population')
# pop_search_filtered = pop_search[pop_search['id'].str.len() <= 5] #states all had an id of 5 characters or less
# pop_search_filtered.head(3)



# #putting 52 df's into a list and then forming population_df
# all_results=[]

# for myid in pop_search_filtered.index:
#     result=fred.get_series(myid) #get series is how we get data in fred Api
#     result=result.to_frame(name=myid)
#     all_results.append(result)
#     population_df=pd.concat(all_results, axis=1)


# #fixing column headings
# population_headings=pop_search_filtered['title'].to_dict()
# population_df.rename(columns=population_headings, inplace=True)



# Check if the cached file exists
try:
    population_df = joblib.load('cached_population_dataframe.joblib')
except FileNotFoundError:
  

    pop_search = fred.search('Resident Population')
    pop_search_filtered = pop_search[pop_search['id'].str.len() <= 5]

    all_results = []
    for myid in pop_search_filtered.index:
        result = fred.get_series(myid)
        result = result.to_frame(name=myid)
        all_results.append(result)
    population_df = pd.concat(all_results, axis=1)

    # Fixing column headings
    population_headings = pop_search_filtered['title'].to_dict()
    population_df.rename(columns=population_headings, inplace=True)

    # Save the DataFrame to a file
    joblib.dump(population_df, 'cached_population_dataframe.joblib')


population_df.head(3)






# #Median Household Income for each state
# socio_search=fred.search('median household income',filter=('units','Current%20Dollars')) #this is going to be for states, I will do for counties next, %20 is ascii for space
# socio_search_filtered_df = socio_search[socio_search['title'].str.contains('Median Household Income in', case=True)] # this is going to be for states, I will do for counties next



# med_results=[]

# for myid in socio_search_filtered_df.index:
#     result=fred.get_series(myid) #get series is how we get data in fred Api
#     result=result.to_frame(name=myid)
#     med_results.append(result)
#     states_mhhi_df=pd.concat(med_results, axis=1)


# title_dict = socio_search_filtered_df["title"].to_dict()
# title_dict

# states_mhhi_df.rename(columns=title_dict, inplace=True) #renaming columns to state names
# states_mhhi_df.head(3)


try:
    states_mhhi_df = joblib.load('cached_states_mhhi_df.joblib')
except FileNotFoundError:
    #Median Household Income for each state
    socio_search=fred.search('median household income',filter=('units','Current%20Dollars')) #this is going to be for states, I will do for counties next, %20 is ascii for space
    socio_search_filtered_df = socio_search[socio_search['title'].str.contains('Median Household Income in', case=True)] # this is going to be for states, I will do for counties next
    med_results=[]

    for myid in socio_search_filtered_df.index:
        result=fred.get_series(myid) #get series is how we get data in fred Api
        result=result.to_frame(name=myid)
        med_results.append(result)
        states_mhhi_df=pd.concat(med_results, axis=1)

    title_dict = socio_search_filtered_df["title"].to_dict()
    title_dict

    states_mhhi_df.rename(columns=title_dict, inplace=True) #renaming columns to state names
    
    # Save the DataFrame to a file
    joblib.dump(states_mhhi_df, 'cached_states_mhhi_df.joblib')
states_mhhi_df.head(3)






# countyHHI=fred.search('median household income') 
# countyHHI_filtered= countyHHI[countyHHI['title'].str.contains('Estimate of Median Household Income', case=True)]
# countyHHI_filtered.head(3)


#fitering out data we dont want (90% confidence interval estimate)
# bad_data=countyHHI_filtered.loc[countyHHI_filtered['title'].str.contains('90%')]
# bad_data['title'].to_dict()



# countyHHI_filtered.drop(bad_data.index, inplace=True)



#takes 43 minutes 20 seconds to run
# county_results=[]

# for myid in countyHHI_filtered.index:
#     import time

#     retries = 0
#     max_retries = 5

#     while retries < max_retries:
#         try:
#             # Make API request here
#             result = fred.get_series(myid)
#             result = result.to_frame(name=myid)
#             county_results.append(result)
#             scountyHHI_filtered_df = pd.concat(county_results, axis=1)
#             time.sleep(3)
#             break  # If successful, exit the loop
#         except RateLimitError:  # Replace with the actual exception type
#             retries += 1
#             delay = 2**retries  # Exponential backoff
#             time.sleep(delay)



# county_series=countyHHI_filtered.index.to_list()

# socio_title = countyHHI_filtered["title"].to_dict()

# scountyHHI_filtered_df.rename(columns=socio_title, inplace=True) #renaming columns to county names
#scounty.to_csv('your_data.csv', index=True)


#made it a csv file after running once previously
county_HHI_df=pd.read_csv('HHIcounty')
county_HHI_df.head(3)





# unemployment_data=fred.search('unemployment rate monthly',filter=('seasonal_adjustment','Not%20Seasonally%20Adjusted'))
# unemployment_data=unemployment_data.query("frequency=='Monthly' and units=='Percent'")
# unemployment_data= unemployment_data[unemployment_data['id'].str.len() <= 5]
# unemployment_data.shape


# unem_results=[]

# for myid in unemployment_data.index:
#     result=fred.get_series(myid) #get series is how we get data in fred Api
#     result=result.to_frame(name=myid)
#     unem_results.append(result)
#     unemployment_df_state=pd.concat(unem_results, axis=1)


# title_dict = unemployment_data["title"].to_dict()


# unemployment_df_state.rename(columns=title_dict, inplace=True) #renaming columns to state names
# unemployment_df_state


#Getting total unemployment data for the US Not Seasonally Adjusted
# fred.search('UNRATENSA')
# non_sa_unem=fred.get_series('UNRATENSA')
# usa_unemployment=non_sa_unem.to_frame(name='US Unemployment Rate Not Seasonally Adjusted')
# usa_unemployment





#merging US unemployment data with state unemployment data
# unem_df=pd.merge(unemployment_df_state, usa_unemployment, left_index=True, right_index=True)
# unem_df.head(3)


# Check if the cached file exists
try:
    unem_df = joblib.load('cached_unemployment_dataframe.joblib')
except FileNotFoundError:
    unemployment_data = fred.search('unemployment rate monthly', filter=('seasonal_adjustment', 'Not%20Seasonally%20Adjusted'))
    unemployment_data = unemployment_data.query("frequency=='Monthly' and units=='Percent'")
    unemployment_data = unemployment_data[unemployment_data['id'].str.len() <= 5]

    unem_results = []
    for myid in unemployment_data.index:
        result = fred.get_series(myid)
        result = result.to_frame(name=myid)
        unem_results.append(result)
    unemployment_df_state = pd.concat(unem_results, axis=1)

    title_dict = unemployment_data["title"].to_dict()
    unemployment_df_state.rename(columns=title_dict, inplace=True)

    fred.search('UNRATENSA')
    non_sa_unem = fred.get_series('UNRATENSA')
    usa_unemployment = non_sa_unem.to_frame(name='US Unemployment Rate Not Seasonally Adjusted')

    unem_df = pd.merge(unemployment_df_state, usa_unemployment, left_index=True, right_index=True)

    # Save the DataFrame to a file
    joblib.dump(unem_df, 'cached_unemployment_dataframe.joblib')


unem_df.head(3)


#Gathering Food Stamp Data reciepients, SNAP


try:
    snap_df = joblib.load('cached_snap_dataframe.joblib')
except FileNotFoundError:
    snap_data=fred.search('Number of Food Stamp Recipients',filter=('frequency','Monthly'))
    snap_results=[]

    for my_id in snap_data.index:
        result=fred.get_series(my_id)
        result=result.to_frame(name=my_id)
        snap_results.append(result)
        snap_df=pd.concat(snap_results, axis=1)
        
        
    title_dict = snap_data["title"].to_dict()
    snap_df.rename(columns=title_dict, inplace=True)
    # Save the DataFrame to a file
    joblib.dump(snap_df, 'cached_snap_dataframe.joblib')
    
snap_df.head(3)






#filting data to only include 2016-2022 and making a date column
population_df.reset_index(inplace=True)
population_df = population_df[population_df['index'].dt.year >= 2016]
population_df.rename(columns={'index':'Date'}, inplace=True)
population_df.reset_index(drop=True,inplace=True)

population_df.head(3)





county_HHI_df.rename(columns={'Unnamed: 0':'Date'}, inplace=True) 
county_HHI_df["Date"] = pd.to_datetime(county_HHI_df["Date"])
county_HHI_df = county_HHI_df[county_HHI_df['Date'].dt.year >= 2016]
county_HHI_df.reset_index(drop=True,inplace=True)


#cleaned county_HHI_df
county_HHI_df.copy().head(3)





unem_df.reset_index(inplace=True)
unem_df = unem_df[unem_df['index'].dt.year >= 2016]
unem_df.rename(columns={'index':'Date'}, inplace=True)
unem_df.reset_index(inplace=True)
unem_df.drop('index', axis=1, inplace=True)


unem_df


missing_values = unem_df.loc[:, unem_df.isna().any()]
print(missing_values)


#dropping puerto rico because it has a lot of missing values
unem_df.drop(columns=['Unemployment Rate in Puerto Rico'], inplace=True)



unem_df['Year-Month']=pd.to_datetime(unem_df['Date']).dt.to_period('M')


unem_df['Year-Month']


#unemployment data is cleaned now for our purposes
c_unem_df=unem_df.copy()






#filting data to only include 2016-2022 and makeing a date column
snap_df.reset_index(inplace=True)
snap_df = snap_df[snap_df['index'].dt.year >= 2016]
snap_df.rename(columns={'index':'Date'}, inplace=True)
snap_df.reset_index(drop=True,inplace=True)

snap_df.head(3)








# interactive_graph=px.scatter_mapbox(car_crash_df,
#                     lat="Start_Lat",
#                     lon="Start_Lng",
#                     color="Severity",
#                     zoom=3,
#                     hover_name="City",
#                     hover_data=["State","Temperature(F)","Humidity(%)","Pressure(in)","Visibility(mi)"],
#                     title="US Accidents Map")
                       
# interactive_graph.update_layout(mapbox_style="open-street-map")
# interactive_graph.show()


#5. Severity and Types of Accidents:
#What types or severity of accidents were most common?

import matplotlib.pyplot as plt

severity_counts = car_crash_df['Severity'].value_counts()

ax = severity_counts.plot(kind='barh')

for i, v in enumerate(severity_counts):
    ax.text(v + 10, i, str(v), color='black', fontweight='bold')


plt.xlabel('Car Accidents')
plt.ylabel('Severity')
plt.title('Car Crashes by Severity')

plt.show()



car_crash_df['Severity'].value_counts() 


#to do
#1 group by location using knn, population, weather patterns
#Question 3 ANSWER: have car accidented decreased over time?,
#Question 4 ANSWER: #get unemployment data, median household income
#6 look at years as a whole, pick certaint counties/states/zip codes to look at indivudlalyl and highlight the changes 


state_counts = car_crash_df['State'].value_counts()
plt.bar(state_counts.index, state_counts.values, color='skyblue')
plt.xlabel('State')
plt.ylabel('Number of Car Crashes')
plt.title('Car Crashes by State')



# state_counts = car_crash_df['State'].value_counts().reset_index()
# state_counts.columns = ['State', 'Number of Car Crashes']
# fig = px.bar(state_counts, x='State', y='Number of Car Crashes', title='Car Crashes by State',
#              labels={'Number of Car Crashes': 'Number of Car Crashes'})
# fig.show()


# import pandas as pd
# import plotly.express as px

# filtered_df = car_crash_df[car_crash_df['Severity'].isin([3, 4])]
# state_counts = filtered_df['State'].value_counts().reset_index(name='Number of Car Crashes')
# fig = px.bar(state_counts, x='State', y='Number of Car Crashes',
#              title='Car Crashes by State for Severity 3 and 4',
#              labels={'State': 'State', 'Number of Car Crashes': 'Number of Car Crashes'})
# fig.show()



# Deep dive into California heat map, showing concentration of accidents within the State
# california_df = car_crash_df[car_crash_df['State'] == 'CA']
# px.set_mapbox_access_token('your_mapbox_token')
# heat_map_concentration = px.density_mapbox(california_df,
#                               lat='Start_Lat',
#                               lon='Start_Lng', # or we can do quanitity of crashes within that area or zip code
#                               radius=10, 
#                               zoom=6, 
#                               mapbox_style='open-street-map',
#                               title='Interactive Heat Map of Car Accidents in California',
#                               hover_name='City',
#                               hover_data=["Temperature(F)", "Humidity(%)", "Pressure(in)", "Visibility(mi)"])

# # Show the heat map
# heat_map_concentration.show()




california_df = car_crash_df[car_crash_df['State'] == 'CA'].copy()
california_df['Crashes_In_Zip'] = california_df.groupby('Zipcode')['Zipcode'].transform('count')
california_df[['Zipcode', 'Crashes_In_Zip']]


#How do i show number of car crashes per capita of population.
# Can we add a column called population which shows the current population of each state?

population_df["Resident Population in California"]



state_to_abbrev = {
    'California': 'CA',
    'Florida': 'FL',
    'New York': 'NY',
    'Georgia': 'GA',
    'Ohio': 'OH',
    'Missouri': 'MO',
    'Michigan': 'MI',
    'Indiana': 'IN',
    'Illinois': 'IL',
    'Connecticut': 'CT',
    'Pennsylvania': 'PA',
    'Massachusetts': 'MA',
    'South Carolina': 'SC',
    'Wisconsin': 'WI',
    'New Jersey': 'NJ',
    'Rhode Island': 'RI',
    'West Virginia': 'WV',
    'New Hampshire': 'NH',
    'Iowa': 'IA',
    'Nebraska': 'NE'}


pop_2022 = population_df[population_df['Date'] == '2022-01-01']
pop_2022


crashes_per_capita_data = {
    'CA': state_counts['CA'] / pop_2022['Resident Population in California'],
    'FL': state_counts['FL'] / pop_2022['Resident Population in Florida'],
    'NY': state_counts['NY'] / pop_2022['Resident Population in New York'],
    'GA': state_counts['GA'] / pop_2022['Resident Population in Georgia'],
    'OH': state_counts['OH'] / pop_2022['Resident Population in Ohio'],
    'MO': state_counts['MO'] / pop_2022['Resident Population in Missouri'],
    'MI': state_counts['MI'] / pop_2022['Resident Population in Michigan'],
    'IN': state_counts['IN'] / pop_2022['Resident Population in Indiana'],
    'IL': state_counts['IL'] / pop_2022['Resident Population in Illinois'],
    'CT': state_counts['CT'] / pop_2022['Resident Population in Connecticut'],
    'PA': state_counts['PA'] / pop_2022['Resident Population in Pennsylvania'],
    'MA': state_counts['MA'] / pop_2022['Resident Population in Massachusetts'],
    'SC': state_counts['SC'] / pop_2022['Resident Population in South Carolina'],
    'WI': state_counts['WI'] / pop_2022['Resident Population in Wisconsin'],
    'NJ': state_counts['NJ'] / pop_2022['Resident Population in New Jersey'],
    'RI': state_counts['RI'] / pop_2022['Resident Population in Rhode Island'],
    'WV': state_counts['WV'] / pop_2022['Resident Population in West Virginia'],
    'NH': state_counts['NH'] / pop_2022['Resident Population in New Hampshire'],
    'IA': state_counts['IA'] / pop_2022['Resident Population in Iowa'],
    'NE': state_counts['NE'] / pop_2022['Resident Population in Nebraska']
}

# Create a dataframe from the computed data
crashes_per_capita_df = pd.DataFrame(list(crashes_per_capita_data.items()), columns=['State', 'Crashes Per Capita'])

# Print the dataframe
## On average, for every thousand residents in [state], there were about [value] crashes. 
print(crashes_per_capita_df)


import pandas as pd

# Provided dictionary
data = {
    'CA': 2.954623,
    'FL': 1.357305,
    'NY': 0.158255,
    'GA': 1.053434,
    'OH': 0.064137,
    'MO': 0.180966,
    'MI': 1.527788,
    'IN': 0.003951,
    'IL': 0.825304,
    'CT': 0.129061,
    'PA': 0.000077,
    'MA': 0.16915,
    'SC': 0.424599,
    'WI': 0.006109,
    'NJ': 0.081303,
    'RI': 0.188346,
    'WV': 0.001127,
    'NH': 0.019352,
    'IA': 0.443991,
    'NE': 2.997068
}

# Convert the dictionary to a DataFrame
crashes_per_capita_df = pd.DataFrame(list(data.items()), columns=['State', 'Crashes Per Capita'])

# Print the DataFrame
print(crashes_per_capita_df)


# Sort the dataframe by 'Crashes Per Capita' for better visualization
crashes_per_capita_df_sorted = crashes_per_capita_df.sort_values(by='Crashes Per Capita', ascending = False)
crashes_per_capita_df_sorted
# Create a bar graph
plt.figure(figsize=(15, 10))
plt.bar(crashes_per_capita_df_sorted['State'], crashes_per_capita_df_sorted['Crashes Per Capita'], color='skyblue')
plt.xlabel('State')
plt.ylabel('Crashes Per Capita')
plt.title('2022 Crashes Per Capita by State')
plt.tight_layout()
plt.show()


# Sort the dataframe by 'Crashes Per Capita' for better visualization
crashes_per_capita_df_sorted = crashes_per_capita_df.sort_values(by='Crashes Per Capita', ascending=False)

# Create an interactive bar graph using Plotly Express
fig = px.bar(crashes_per_capita_df_sorted, 
             x='State', 
             y='Crashes Per Capita', 
             color='Crashes Per Capita',
             color_continuous_scale='Blues',  # You can change the color scale as needed
             title='2022 Crashes Per Capita by State')

fig.update_layout(
    xaxis_title='State',
    yaxis_title='Crashes Per Capita (1,000 Residents)',
    template='plotly_white',  # Choose a template, e.g., 'plotly_dark' for dark theme
    height=500,  # Adjust the height as needed
    width=900   # Adjust the width as needed
)

fig.show()


## we can use this to create a chart showing accidents per month by year to see the impacts during covid 

crash_date_df = car_crash_df
startend = crash_date_df[['Severity','Start_Time', 'End_Time', 'Weather_Condition']]
startend['Start_Time'] = pd.to_datetime(startend['Start_Time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')
startend['newtime'] = startend['Start_Time'].dt.to_period('M').astype(str)
startend








##this creates seperate colums for year and month
startend['year'] = startend['Start_Time'].dt.year
startend['month'] = startend['Start_Time'].dt.month
startend.head(4)


crashes_per_month = startend.groupby(['year', 'month']).size().reset_index(name='count')

plt.figure(figsize=(12, 8))

for year in crashes_per_month['year'].unique():
    data_year = crashes_per_month[crashes_per_month['year'] == year]
    plt.plot(data_year['month'], data_year['count'], label=str(year), marker='o', linestyle='-')

plt.title('Number of Crashes per Month (2016-2023)')
plt.xlabel('Month')
plt.ylabel('Number of Crashes')
plt.legend(title='Year', loc='upper right', bbox_to_anchor=(1.3, 1))
plt.show()

#this version of code makes it interactive
# fig = px.line(crashes_per_month, x='month', y='count', color='year', markers=True,
#                labels={'count': 'Number of Crashes', 'month': 'Month', 'year': 'Year'},
#                title='Number of Crashes per Month (2016-2023)')

# fig.show()



def map_to_season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    else:
        return 'Fall'

startend['season'] = startend['month'].apply(map_to_season)
startend.tail()


# Assuming 'startend' is your DataFrame
weather_severity_totals = startend.groupby(['Weather_Condition', 'Severity']).size().reset_index(name='Total Accidents')

fig = px.scatter(weather_severity_totals, x='Weather_Condition', y='Total Accidents',
                 size='Severity',  # Set marker size based on Severity
                 color='Severity',
                 labels={'Weather_Condition': 'Weather Condition', 'Total Accidents': 'Number of Accidents'},
                 title='Bubble Chart: Accidents by Weather Condition, Number of Accidents, and Severity')

fig.show()


severity_by_season = startend.groupby('season')['Severity'].mean()
severity_by_season


#did they improve or get worse over time
#highlight specific months 


#thoughts

#use Knn imputation for data >1, average for data <1
#explore metadata, maybe drop columns too help with run time
#graphs showing car accidents overtime
#clean sunrise_sunset column



#to do
#1  population, weather patterns
#Question 3 ANSWER: have car accidents decreased over time?,
#Question 4 ANSWER: #get unemployment data, median household income
#6 look at years as a whole, pick certaint counties/states/zip codes to look at indivudlalyl and highlight the changes 





#calculating correlation between unemployment and car crashes

# merged_df = pd.merge(c_unem_df, car_crash_df, on='Year-Month', how='inner')
# merged_df


#Did unemployment have a positive or negative correlation with car accidents?
merged_df.columns


#merged_df = pd.merge(unemployment_df, car_crash_df, on='date', how='inner')


#Do various areas with low socioeconomic status and income affect car accidents in any way? (correlation between the amount or severity of accidents and income/demographics)





# Uniformly fill resident population data for each month for the given years data
start_date = pd.to_datetime('2016-01-01')
end_date = pd.to_datetime('2022-01-01')

monthly_dates = pd.date_range(start=start_date, end=end_date, freq='MS')
population_df.set_index('Date', inplace=True)
expanded_df = population_df.reindex(monthly_dates)
expanded_df.ffill(inplace=True)


#making date column for expanded_df
expanded_df.reset_index(inplace=True)
expanded_df.rename(columns={'index':'Date'}, inplace=True)



#since FRED API measures Population in thousands, we need to multiply by 1000 to get the actual population

expanded_df.loc[:, expanded_df.columns != 'Date'] *= 1000
expanded_df.loc[:, expanded_df.columns != 'Date'] = expanded_df.loc[:, expanded_df.columns != 'Date'].round(0).astype(int)



#Droping columns not needed for analysis in both df's
expanded_df.drop(columns=['Resident Population in Federal Reserve District 1: Boston','Resident Population in Federal Reserve District 9: Minneapolis'], inplace=True)
snap_df.drop("SNAP Benefits Recipients in District of Columbia",axis=1, inplace=True)
expanded_df.drop("Resident Population in the District of Columbia", axis=1, inplace=True)


snap_df.shape


#merging snap data with population data
snap_with_pop=pd.merge(snap_df, expanded_df, on='Date', how='inner')
snap_with_pop.tail(3)
#snap_with_pop['California SNAP Beneifts Per 100,000'] = (snap_with_pop['SNAP Benefits Recipients in California'] / snap_with_pop['Resident Population in California']) * 100000

# snap_with_pop.str.contains('California')
#normalizing snap data





state_lst = expanded_df.columns.str.split(' ')
state_lst = state_lst[1:] #removing Date from my state_lst

state_lst = [state[3] if len(state) < 4 else state[3:5] for state in state_lst] #extracting state name for 4 columns

state_lst


#joining together 2 word states in one string 
state_lst = [' '.join(state) if type(state) == list else state for state in state_lst]
state_lst #out state_lst is complete


# Create an empty dictionary to store the results
snap_benefits_per_capita = {}

# Loop through each state
for state in state_lst:
    
    # Divide one column by the other
    result = snap_with_pop['SNAP Benefits Recipients in ' + state] / snap_with_pop['Resident Population in ' + state] 
    
    # Store the result in the dictionary
    snap_benefits_per_capita[state] = result


snap_benefits_per_capita


snap_benefits_per_capita_df = pd.DataFrame(snap_benefits_per_capita)




#rounding to nearest whole number
snap_benefits_per_capita_df = (snap_benefits_per_capita_df * 100000).round()



snap_benefits_per_capita_df


#finding the 10 lowest and 10 highest states for snap benefits per capita
top_10_highest_snap=snap_benefits_per_capita_df.mean().round().sort_values(ascending=False).head(10)
top_10_highest_snap


top_10_highest_snap.plot(kind='barh', title='Top 10 Highest States for SNAP Benefits Recipients Per 100,000', xlabel='Number of SNAP Benefits Recipients Per 100,000', ylabel='State')
plt.show()


top_10_lowest_snap=snap_benefits_per_capita_df.mean().round().sort_values(ascending=True).head(10)
top_10_lowest_snap


top_10_lowest_snap.plot(kind='barh', title='Top 10 lowest States for SNAP Benefits Recipients Per 100,000', xlabel='Number of SNAP Benefits Recipients Per 100,000', ylabel='State')
plt.show()


import matplotlib.pyplot as plt



# Combine the two DataFrames
combined_df = pd.concat([top_10_highest_snap, top_10_lowest_snap]).sort_values(ascending=False)

# Plot the bar graph
colors = ['skyblue'] * len(top_10_highest_snap) + ['lightgreen'] * len(top_10_lowest_snap)
plt.barh(combined_df.index, combined_df.values, color=colors)

plt.xlabel('Index')
plt.ylabel('Values')
plt.title('Top 10 Highest and Lowest Values 2016-2022')
plt.show()




#grouping car crash data by state and month and then then based on snap data see if there is a difference
grouping_car_crash_data=car_crash_df.groupby(['State','Year-Month']).size().reset_index(name='Number of Car Crashes')


grouped_car_crash_data =grouping_car_crash_data.pivot(index='Year-Month', columns='State', values='Number of Car Crashes')
grouped_car_crash_data



#trimming data to only include 2016-2022
grouped_car_crash_data = grouped_car_crash_data.loc['2016-01':'2022-01']
grouped_car_crash_data


#dealing with missing values by filling them with the average of the column
grouped_car_crash_data.fillna(grouped_car_crash_data.mean(), inplace=True)
grouped_car_crash_data


grouped_car_crash_data.columns#=new_state_lst


#combined df has the states we need to extract from grouped_car_crash_data
combined_df = pd.DataFrame(combined_df)

combined_df.reset_index(inplace=True)
combined_df.rename(columns={'index':'State'}, inplace=True)



#understanding what states I need
states_for_analysis=combined_df['State']
states_for_analysis


#with the help of an AI friend, I was able to get the states I needed for my analysis

states_for_analysis = {
    "NM": "New Mexico",
    "LA": "Louisiana",
    "WV": "West Virginia",
    "MS": "Mississippi",
    "OR": "Oregon",
    "AL": "Alabama",
    "OK": "Oklahoma",
    "GA": "Georgia",
    "FL": "Florida",
    "IL": "Illinois",
    "NJ": "New Jersey",
    "ID": "Idaho",
    "NE": "Nebraska",
    "CO": "Colorado",
    "MN": "Minnesota",
    "KS": "Kansas",
    "ND": "North Dakota",
    "NH": "New Hampshire",
    "UT": "Utah",
    "WY": "Wyoming"
}


grouped_car_crash_data = grouped_car_crash_data[states_for_analysis.keys()]


grouped_car_crash_data.rename(columns=states_for_analysis, inplace=True)



grouped_car_crash_data.to_csv('grouped_car_crash_data.csv')


grouped_car_crash_df=pd.read_csv('grouped_car_crash_data.csv')


#repeating this for population data
population_subset_analysis = expanded_df[states_for_analysis.keys()]





#two samplemm t -test between california and connecticut car accidents
#word cloud of most common words in description






#wordcloud of most common words in description
# pd.set_option('display.max_colwidth', None) #helping me visualize the description column
#fix word cloud becuase it buggs with lots of data

car_crash_df['Description']

# Concatenate all the descriptions into a single string
text = ' '.join(car_crash_df['Description'])

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Most Common Words in Description')
plt.show()



car_crash_df['Street'].value_counts().head(10).plot(kind='barh')
plt.xlabel('Highway')
plt.ylabel('Number of Car Crashes')
plt.title("Top 10 Highways with the Most Car Crashes")
plt.show()






print('Car accidents that occurred with and without nearby speed bumps')
raw_bump_data = car_crash_df["Bump"].value_counts()
total_accidents = len(car_crash_df)
bump_percentages = raw_bump_data / total_accidents * 100

print("(Speed Bump):", raw_bump_data.iloc[1], f"({bump_percentages.iloc[1]:.2f})%")
print("(No Speed Bump):", raw_bump_data.iloc[0], f"({bump_percentages.iloc[0]:.2f})%")



#further investigation into speed bumps and car accidents
car_crashes_with_bumps = car_crash_df[car_crash_df["Bump"] == True]


#plotting interactive map of car crashes with speed bumps
interactive_graph=px.scatter_mapbox(car_crashes_with_bumps,
                    lat="Start_Lat",
                    lon="Start_Lng",
                    color="Severity",
                    zoom=3,
                    hover_name="City",
                    hover_data=["State","Temperature(F)","Visibility(mi)"],
                    title="US Accidents with Speed Bumps Nearby")
interactive_graph.update_layout(mapbox_style="open-street-map")
interactive_graph.show()


car_crashes_with_bumps["Severity"].value_counts()


# t-test of if severity of car accidents severity is affected by speed bumps


#graphing car crashes with speed bumps by weather patterns


car_crashes_with_bumps["Weather_Condition"].value_counts()


#more analysis on car crashes with speed bumps with weather patterns 


6. Regional Improvements: Which areas have improved the most since 2016 at decreasing the rate of car accidents?


car_crash_df.head()


stateswithregions = car_crash_df[['Severity', 'State', 'Year-Month']].copy()
stateswithregions


## mapping states to regions based on US census region map

region_to_states_mapping = {
    'West': ['AK', 'AZ', 'CA', 'CO', 'HI', 'ID', 'MT', 'NV', 'NM', 'OR', 'UT', 'WA', 'WY'],
    'Midwest': ['IL', 'IN', 'IA', 'KS', 'MI', 'MN', 'MO', 'NE', 'ND', 'OH', 'SD', 'WI'],
    'South': ['AL', 'AR', 'DC', 'DE', 'MD', 'FL', 'GA', 'KY', 'LA', 'MS', 'NC', 'OK', 'SC', 'TN', 'TX', 'WV'],
    'Northeast': ['CT', 'MA', 'ME', 'NH', 'NJ', 'NY', 'PA', 'RI', 'VT']
}

# Create a reverse mapping from state to region
state_to_region_mapping = {state: region for region, states in region_to_states_mapping.items() for state in states}

stateswithregions['Region'] = stateswithregions['State'].map(state_to_region_mapping)


stateswithregions


## put year and month into their own columns
stateswithregions['Year'] = stateswithregions['Year-Month'].dt.year
stateswithregions['Month'] = stateswithregions['Year-Month'].dt.month
stateswithregions


# Assuming 'stateswithregions' is your DataFrame
region_year_totals = stateswithregions.groupby(['Region', 'Year']).size().reset_index(name='Total Accidents')

plt.figure(figsize=(12, 6))

# Plot lines for each region
for region in region_year_totals['Region'].unique():
    region_data = region_year_totals[region_year_totals['Region'] == region]
    plt.plot(region_data['Year'], region_data['Total Accidents'], marker='o', label=region)

    # # Annotate each point with its value
    for i, txt in enumerate(region_data['Total Accidents']):
        plt.annotate(txt, (region_data['Year'].iloc[i], region_data['Total Accidents'].iloc[i]))

plt.title('Total Car Accidents per Year for Each Region')
plt.xlabel('Year')
plt.ylabel('Total Accidents')
plt.legend(title='Region')
plt.show()









